import asyncio
import json
import os
import datetime
import aiofiles
from ast import literal_eval
from collections import deque
from os import PathLike
from typing import Generator, Dict, Union, Optional, AsyncGenerator, Callable, Any, Deque
from datetime import datetime


# __all__ = ["get_files", "name_file", "generate_log_header", "check_directory", ]
__all__ = ['JSONEventProcessor', 'get_json_processor', 'load_events']

def get_files(
        directory: Union[str, PathLike[str]],
        max_depth: int = 2,
        absolute: bool = False
) -> Generator[Dict[str, Union[str, None]], None, None]:
    """
    éå†æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼Œç”ŸæˆåŒ…å«ä¿¡æ¯çš„å­—å…¸ç”Ÿæˆå™¨

    :param directory: è¦éå†çš„æ ¹ç›®å½•è·¯å¾„ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ–PathLikeå¯¹è±¡ï¼‰
    :param max_depth: æœ€å¤§åµŒå¥—å±‚æ•°ï¼ˆé»˜è®¤2å±‚ï¼‰
    :param absolute: æ˜¯å¦è¿”å›ç»å¯¹è·¯å¾„ï¼ˆé»˜è®¤Falseè¿”å›ç›¸å¯¹è·¯å¾„ï¼‰
    :yield: åŒ…å«pathã€nameã€suffixå’ŒDiskï¼ˆWindowsï¼‰çš„å­—å…¸
    """
    # ç±»å‹å®‰å…¨å¤„ç†è¾“å…¥è·¯å¾„
    safe_directory = str(directory)
    start_directory = os.path.abspath(safe_directory)
    is_windows = os.name == 'nt'

    for root, dirs, files in os.walk(start_directory):
        # è®¡ç®—å½“å‰è·¯å¾„ä¿¡æ¯
        current_path = os.path.abspath(root)
        rel_path = os.path.relpath(current_path, start_directory)

        # è®¡ç®—å½“å‰æ·±åº¦
        current_depth = 0 if rel_path == '.' else len(rel_path.split(os.path.sep))

        # æ·±åº¦æ§åˆ¶é€»è¾‘
        if current_depth > max_depth:
            del dirs[:]  # é˜»æ­¢ç»§ç»­æ·±å…¥éå†
            continue

        # å¤„ç†å­ç›®å½•
        for dir_name in dirs[:]:  # ä½¿ç”¨å‰¯æœ¬éå†
            # æ„å»ºå®‰å…¨è·¯å¾„å¹¶å¤„ç†ç±»å‹
            subdir_path = os.path.join(root, dir_name)
            safe_subdir = str(subdir_path)  # å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²

            # è®¡ç®—ç»å¯¹è·¯å¾„å’Œç›˜ç¬¦
            abs_subdir_path = os.path.abspath(safe_subdir)
            drive, path_part = os.path.splitdrive(abs_subdir_path)

            # æ„é€ è¿”å›è·¯å¾„
            if absolute:
                path = path_part if is_windows else abs_subdir_path
            else:
                path = os.path.relpath(abs_subdir_path, start_directory)

            # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦
            normalized_path = path.replace(os.sep, '/')

            # æ„å»ºæ¡ç›®
            entry = {
                'path': normalized_path,
                'name': dir_name,
                'suffix': 'folder'
            }
            if is_windows:
                entry['Disk'] = drive
            yield entry

        # å¤„ç†æ–‡ä»¶
        for filename in files:
            # æ„å»ºå®‰å…¨è·¯å¾„
            full_path = os.path.join(root, filename)
            safe_full_path = str(full_path)  # å¼ºåˆ¶ç±»å‹è½¬æ¢

            # è®¡ç®—ç»å¯¹è·¯å¾„å’Œç›˜ç¬¦
            abs_full_path = os.path.abspath(safe_full_path)
            drive, path_part = os.path.splitdrive(abs_full_path)

            # æ„é€ è¿”å›è·¯å¾„
            if absolute:
                path = path_part if is_windows else abs_full_path
            else:
                path = os.path.relpath(abs_full_path, start_directory)

            # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦
            normalized_path = path.replace(os.sep, '/')

            # åˆ†å‰²æ–‡ä»¶åå’Œåç¼€
            if '.' in filename:
                name_part, suffix_part = filename.split('.', 1)
            else:
                name_part = filename
                suffix_part = None

            # æ„å»ºæ¡ç›®
            entry = {
                'path': normalized_path,
                'name': name_part,
                'suffix': suffix_part
            }
            if is_windows:
                entry['Disk'] = drive
            yield entry


# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     # é»˜è®¤æ—¥æœŸæ ¼å¼ + åç¼€
#     print(name_file(suffix="log"))  # è¾“å‡ºç±»ä¼¼ï¼š07-15_14-30.log
#
#     # è‡ªå®šä¹‰æ—¥æœŸæ ¼å¼
#     print(name_file("date", file_name="%Y%m%d", suffix="data"))  # ç±»ä¼¼ï¼š20230715.data
#
#     # åºå·æ¨¡å¼å¸¦åç¼€
#     print(name_file("number", suffix=".txt"))  # file_0001.txt
#     print(name_file("number", suffix="bak"))   # file_0002.bak
#
#     # ç›´æ¥å‘½åæ¨¡å¼
#     print(name_file("name", file_name='"report"', suffix="pdf"))  # report.pdf
#     print(name_file("name", file_name="'data'", suffix="csv"))    # data.csv
#
#     # æ— æ•ˆåç¼€å¤„ç†
#     print(name_file(suffix=123))  # å¿½ç•¥åç¼€ â†’ 07-15_14-30  # è¿™æ®µæ˜¯ç”¨äºæµ‹è¯•é”™è¯¯æƒ…å†µçš„ï¼ŒIDEä¼šæœ‰å¼±è­¦å‘Šå¾ˆæ­£å¸¸
#     print(name_file(suffix=True)) # å¿½ç•¥ â†’ 07-15_14-30


_NAMED_COUNTERS = {}  # å…¨å±€è®¡æ•°å™¨å­˜å‚¨å­—å…¸

def name_file(
        mode: str = "date",
        counter_name: str = "NAMED_NUM",
        file_name: Optional[str] = None,
        suffix: Union[bool, str] = False
) -> Optional[str]:
    """
    å¤šåŠŸèƒ½æ–‡ä»¶åç”Ÿæˆå‡½æ•°

    :param mode: ç”Ÿæˆæ¨¡å¼ - date/number/name
    :param counter_name: è®¡æ•°å™¨åç§°ï¼ˆä»…numberæ¨¡å¼æœ‰æ•ˆï¼‰
    :param file_name: æ ¼å¼å­—ç¬¦ä¸²ï¼ˆdateæ¨¡å¼ï¼‰æˆ–å¸¦å¼•å·æ–‡ä»¶åï¼ˆnameæ¨¡å¼ï¼‰
    :param suffix: æ–‡ä»¶åç¼€ï¼ˆå­—ç¬¦ä¸²ç±»å‹è‡ªåŠ¨å¤„ç†ç‚¹å·ï¼‰
    :return: ç”Ÿæˆçš„æ–‡ä»¶å æˆ– None
    """
    # åˆå§‹åŒ–
    base_name = None  # å†³å®šåŸºç¡€åç§° å¹¶é˜²æ­¢åç»­è®¾è®¡å¿˜è®°æ·»åŠ æœªå®šä¹‰é”™è¯¯çš„å¤„ç†

    # æ—¥æœŸæ¨¡å¼æ”¹è¿›
    if mode == "date":
        fmt = "%m-%d_%H-%M"  # é»˜è®¤æ ¼å¼
        if file_name:
            fmt = file_name  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²ï¼Œåˆ†ç±»try

        # å°è¯•ç”Ÿæˆæ ¼å¼
        try:
            # ä¸¥æ ¼æ•è·æ ¼å¼é”™è¯¯
            base_name = datetime.now().strftime(fmt)
        except ValueError as ve:
            # æ ¼å¼æ— æ•ˆæ—¶å›é€€é»˜è®¤æ ¼å¼
            print(f"Invalid format {fmt}, using default. Error: {ve}")
            base_name = datetime.now().strftime("%m-%d_%H-%M")

    # åºå·æ¨¡å¼
    elif mode == "number":
        if counter_name not in _NAMED_COUNTERS:
            _NAMED_COUNTERS[counter_name] = 0
        _NAMED_COUNTERS[counter_name] += 1
        base_name = f"file_{_NAMED_COUNTERS[counter_name]:04d}"

    # ç›´æ¥å‘½åæ¨¡å¼æ”¹è¿›
    elif mode == "name":
        if not file_name:
            return None

        try:
            # ç²¾ç¡®æ•è·å­—é¢é‡è§£æé”™è¯¯
            parsed_name = literal_eval(file_name)
            if not isinstance(parsed_name, str):
                return None
            base_name = parsed_name
        except (SyntaxError, ValueError) as se:
            # æ˜ç¡®è®°å½•è§£æé”™è¯¯
            print(f"Invalid filename literal: {file_name}. Error: {se}")
            return None

    # å¤„ç†æ— æ•ˆæ¨¡å¼
    else:
        return base_name  # åŸè®¾è®¡æ˜¯ç›´æ¥è¿”å›None ä½†æ˜¯è¿™ä¼šå¯¼è‡´åˆå§‹åŒ–base_nameæ—¶å€™çš„å˜é‡æœªä½¿ç”¨

    # å¤„ç†åç¼€
    if isinstance(suffix, str):
        suffix = suffix.strip()
        if suffix:
            # è‡ªåŠ¨è¡¥å…¨ç‚¹å·
            if not suffix.startswith("."):
                suffix = f".{suffix}"
            base_name += suffix

    return base_name


# if __name__ == '__main__':
#     # Windowsæµ‹è¯•
#     print("Windowsæµ‹è¯•ï¼š")
#     for item in get_files('D:\python\MyScripActuator\saves', max_depth=2, absolute=True):
#         print(item)
#
#     # # Linuxæµ‹è¯•
#     # print("\nLinuxæµ‹è¯•ï¼š")
#     # for item in get_files('/home/user/docs', absolute=False):
#     #     print(item)


def generate_log_header(file_path, absolute_path=True, date: (bool, str) = False, date_format="%m-%d_%H-%M"):
    """
    ç”Ÿæˆæ—¥å¿—æ–‡ä»¶çš„å¼€å¤´å†…å®¹

    :param file_path: åˆ¤æ–­æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ é¿å…æ—¥å¿—å†…å®¹ä¸å‡†ç¡®
    :param absolute_path: æ˜¯å¦æ˜¯ç»å¯¹è·¯å¾„
    :param date: éœ€è¦å¡«å…¥çš„æ—¥æœŸ
    :param date_format: æ—¥æœŸæ ¼å¼ ç”¨äºç»Ÿä¸€è°ƒç”¨æ ¼å¼ éå¿…è¦ä¸è¦æ”¹
    :return:
    """
    # å¤„ç†æ–‡ä»¶è·¯å¾„å¹¶ç”Ÿæˆç¬¬ä¸€è¡Œå†…å®¹
    # line1 = None
    if absolute_path:
        # å°è¯•ä½œä¸ºç»å¯¹è·¯å¾„
        abs_path = os.path.abspath(file_path)
        if os.path.exists(abs_path):
            line1 = abs_path
        else:
            # å°è¯•ä½œä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ç›¸å¯¹è·¯å¾„ï¼ˆå‡è®¾é¡¹ç›®æ ¹ç›®å½•ä¸ºå½“å‰å·¥ä½œç›®å½•ï¼‰
            project_root = os.getcwd()
            combined_path = os.path.join(project_root, file_path)
            combined_abs_path = os.path.abspath(combined_path)
            if os.path.exists(combined_abs_path):
                line1 = combined_abs_path
            else:
                return  # æ— æ³•ç”Ÿæˆç¬¬ä¸€è¡Œï¼Œç”Ÿæˆå™¨ç»“æŸ
    else:
        # ç›´æ¥ä½œä¸ºç›¸å¯¹è·¯å¾„å¤„ç†
        if os.path.exists(file_path):
            line1 = file_path
        else:
            return  # æ— æ³•ç”Ÿæˆç¬¬ä¸€è¡Œï¼Œç”Ÿæˆå™¨ç»“æŸ

    # ç”Ÿæˆç¬¬äºŒè¡Œå†…å®¹
    line2 = "\n"
    if isinstance(date, str):
        try:
            # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç¤ºä¾‹æ”¯æŒISOæ ¼å¼å’Œè‡ªå®šä¹‰æ ¼å¼ï¼‰
            # å°è¯•å¸¸è§æ ¼å¼ï¼Œå¦‚ISO 8601
            dt = datetime.fromisoformat(date)
            formatted_date = dt.strftime(date_format)
            line2 = f"{formatted_date}\n"
        except ValueError:
            # å°è¯•å…¶ä»–æ ¼å¼ï¼Œä¾‹å¦‚dateutilè§£æå™¨å¯å¤„ç†æ›´å¤šæ ¼å¼ï¼ˆéœ€å®‰è£…python-dateutilï¼‰
            # æ­¤å¤„ç®€åŒ–å¤„ç†ï¼Œå¯æ ¹æ®éœ€æ±‚æ‰©å±•
            pass

    # ç¬¬ä¸‰è¡Œæš‚æ—¶å›ºå®šæ¢è¡Œ
    line3 = "\n"

    # ä»¥ç”Ÿæˆå™¨å½¢å¼è¿”å›å„è¡Œå†…å®¹
    yield f"{line1}\n"
    yield line2
    yield line3


# print("a")
# # ç¤ºä¾‹1ï¼šç»å¯¹è·¯å¾„å­˜åœ¨ï¼Œdateä¸ºæœ‰æ•ˆæ—¥æœŸå­—ç¬¦ä¸²
# gen = generate_log_header(r"D:\python\MyScripActuator\saves\test\help.txt", date="2023-10-05T14:30:00")
# for line in gen:
#     print(line, end='')
#
# print("b")
# # ç¤ºä¾‹2ï¼šç›¸å¯¹è·¯å¾„å­˜åœ¨ï¼Œdateæ— æ•ˆ
# gen = generate_log_header(r"D:\python\MyScripActuator\saves\test\help.txt", absolute_path=False, date="123")
# for line in gen:
#     print(line, end='')
#
# print("c")
# # ç¤ºä¾‹3ï¼šè·¯å¾„ä¸å­˜åœ¨ï¼Œç”Ÿæˆå™¨æ— è¾“å‡º
# gen = generate_log_header("nonexistent.log")
# print(list(gen))  # è¾“å‡ºç©ºåˆ—è¡¨


def check_directory(path: str, absolute_path: bool = False, create_if_missing: bool = False) -> bool:
    """å…¨å±€ç›®å½•æ£€æµ‹å‡½æ•°"""
    # è·¯å¾„è§£æ
    if not absolute_path:
        base_dir = os.getcwd()
        full_path = os.path.normpath(os.path.join(base_dir, path.lstrip("/")))
    else:
        full_path = path

    # æ£€æŸ¥/åˆ›å»ºç›®å½•
    if os.path.exists(full_path):
        return os.path.isdir(full_path)

    if create_if_missing:
        try:
            os.makedirs(full_path, exist_ok=True)
            return True
        except Exception as e:
            print(f"Directory creation failed: {str(e)}")
            return False
    return False


# ================= è¶…çº§jsonè¯»å–å™¨ =================

# ================= å†…ç½®é™æ€åŠŸèƒ½ =================
def _process_raw_event(raw: Dict) -> Dict:
    """ç»Ÿä¸€äº‹ä»¶å¤„ç†é€»è¾‘ï¼ˆé™æ€æ–¹æ³•ä¼˜åŒ–ï¼‰"""
    if "type" not in raw:
        raise ValueError("Missing required 'type' field in event")

    return {
        "event_type": raw["type"],
        "data": {k: v for k, v in raw.items() if k != "type"}
    }


# ================= å…¨å±€å•ä¾‹ =================
_json_processor_instance = None  # æš‚æ—¶ä¸éœ€è¦ç›´æ¥åˆ›å»ºå®ä¾‹


# ================= è§£æå™¨ç±» =================
class JSONEventProcessor:
    def __init__(self):
        self._file_lock = asyncio.Lock()
        self._event_cache: Deque = deque()
        self._hooks = []
        self._active = True

    async def stream_events(self, path: str) -> AsyncGenerator[Dict[str, Any], None]:
        """æ ¸å¿ƒæµå¼äº‹ä»¶ç”Ÿæˆæ–¹æ³•

        Args:
            path: JSONæ–‡ä»¶è·¯å¾„

        Yields:
            æ ‡å‡†åŒ–äº‹ä»¶å­—å…¸ï¼ˆåŒ…å«event_typeå’Œdataä¸¤ä¸ªé”®ï¼‰

        åŠŸèƒ½ç‰¹ç‚¹ï¼š
            - å¼‚æ­¥æ–‡ä»¶é”ä¿è¯æ–‡ä»¶è¯»å–åŸå­æ€§
            - è‡ªåŠ¨è½¬æ¢åŸå§‹JSONäº‹ä»¶ç»“æ„
            - å®æ—¶ç¼“å­˜å’Œé’©å­è§¦å‘
            - æ”¯æŒæµæš‚åœ/æ¢å¤æ§åˆ¶
        """
        # ä½¿ç”¨å¼‚æ­¥é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªåç¨‹è¯»å–æ–‡ä»¶
        async with self._file_lock:  # ğŸ”’ é˜²æ­¢å¤šä¸ªæ¶ˆè´¹è€…åŒæ—¶è¯»å–æ–‡ä»¶

            # å¼‚æ­¥æ‰“å¼€æ–‡ä»¶ï¼ˆä½¿ç”¨aiofileså®ç°çœŸæ­£çš„å¼‚æ­¥IOï¼‰
            async with aiofiles.open(path, 'r') as f:  # ğŸ“‚ éé˜»å¡æ–‡ä»¶æ“ä½œ

                # åŠ è½½å¹¶è§£æJSONæ•°æ®
                raw_data = json.loads(await f.read())  # â³ å¼‚æ­¥ç­‰å¾…æ–‡ä»¶è¯»å–å®Œæˆ

                # éå†åŸå§‹äº‹ä»¶æ•°æ®
                for raw_event in raw_data:  # ğŸ”„ é€ä¸ªå¤„ç†äº‹ä»¶

                    # æ£€æŸ¥æµæ§åˆ¶çŠ¶æ€
                    if not self._active:  # â¸ï¸ æš‚åœçŠ¶æ€æ£€æµ‹
                        await self._wait_for_resume()  # â³ ç­‰å¾…æ¢å¤

                    # å¤„ç†åŸå§‹äº‹ä»¶æ ¼å¼
                    processed = self._process_raw_event(raw_event)  # ğŸ› ï¸ æ ‡å‡†åŒ–è½¬æ¢
                    if not processed:
                        continue

                    # ç”Ÿæˆäº‹ä»¶ï¼ˆæ ¸å¿ƒäº§å‡ºç‚¹ï¼‰
                    yield processed  # ğŸš€ äº§å‡ºäº‹ä»¶åˆ°è°ƒç”¨æ–¹

                    # æ›´æ–°ç¼“å­˜å¹¶è§¦å‘é’©å­
                    self._event_cache.append(processed)  # ğŸ’¾ å­˜å…¥ç¼“å­˜
                    await self._trigger_hooks()  # ğŸ“¡ é€šçŸ¥æ‰€æœ‰ç›‘å¬è€…

    # ================= å†…ç½®åŠŸèƒ½ =================
    @staticmethod
    def _process_raw_event(raw: Dict) -> Dict:
        """ç»Ÿä¸€äº‹ä»¶å¤„ç†é€»è¾‘"""
        if "type" not in raw:
            raise ValueError("Missing required 'type' field in event")

        return {
            "event_type": raw["type"],
            "data": {k: v for k, v in raw.items() if k != "type"}
        }

    @property
    def is_active(self) -> bool:
        """æµçŠ¶æ€è®¿é—®æ¥å£"""
        return self._active

    @property
    def list_events(self) -> list:
        """å®‰å…¨è®¿é—®ç¼“å­˜å‰¯æœ¬"""
        return list(self._event_cache)

    @property
    def cache_size(self) -> int:
        """ç¼“å­˜æ•°é‡æŸ¥è¯¢"""
        return len(self._event_cache)

    @property
    def cached_events(self) -> tuple:
        """è¿”å›ä¸å¯ä¿®æ”¹çš„ç¼“å­˜å‰¯æœ¬"""
        return tuple(self._event_cache)

    # ================= æ‰©å±•æ§åˆ¶æ¥å£ =================
    def register_hook(self, callback: Callable):
        """æ³¨å†Œé’©å­å‡½æ•°çš„å‚æ•°éªŒè¯"""
        if not callable(callback):
            raise TypeError("é’©å­å¿…é¡»ä¸ºå¯è°ƒç”¨å¯¹è±¡")
        self._hooks.append(callback)

    def clear_cache(self):
        """æ¸…ç©ºäº‹ä»¶ç¼“å­˜"""
        self._event_cache.clear()

    def pause_stream(self):
        """æš‚åœäº‹ä»¶æµï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

        æ•ˆæœï¼š
        - ç«‹å³åœæ­¢åç»­äº‹ä»¶äº§å‡º
        - ä¿æŒå½“å‰çŠ¶æ€ç›´åˆ°resumeè¢«è°ƒç”¨
        """
        self._active = False

    def resume_stream(self):
        """æ¢å¤äº‹ä»¶æµ"""
        self._active = True

    async def _trigger_hooks(self):
        """è§¦å‘å·²æ³¨å†Œçš„é’©å­"""
        for hook in self._hooks:
            if asyncio.iscoroutinefunction(hook):
                await hook(self._event_cache)
            else:
                hook(self._event_cache)

    async def _wait_for_resume(self):
        pass


# ================= è·å–æ–¹å¼ =================
def get_json_processor() -> JSONEventProcessor:
    """è·å–å…¨å±€å•ä¾‹å®ä¾‹"""
    global _json_processor_instance
    if not _json_processor_instance:
        _json_processor_instance = JSONEventProcessor()
    return _json_processor_instance


# ================= ç®€åŒ–ç‰ˆAPI =================
async def load_events(path: str) -> AsyncGenerator[Dict[str, Any], None]:
    """ç®€åŒ–çš„äº‹ä»¶åŠ è½½å…¥å£å‡½æ•°"""
    processor = get_json_processor()
    async for event in processor.stream_events(path):
        yield event


# class LoggerOperator:
#     """æ—¥å¿—ç®¡ç†å™¨"""
#
#     def __init__(self):
#         """æ—¥å¿—æ–‡ä»¶æ ¹ç›®å½•"""
#         self.root_directory = {  # è®°å½•æ—¥å¿—æ–‡ä»¶çš„æ ¹ç›®å½•
#             "main": {  # ä¸»è¦æ—¥å¿—ç›®å½•
#                 "path": "/saves/logs",  # è·¯å¾„
#                 "absolute_path": False,  # æ˜¯å¦æ˜¯ç»å¯¹è·¯å¾„
#                 "currently_processed": name_file(mode="date") # å½“å‰å¤„ç†çš„æ–‡ä»¶
#             },
#             "temp": {  # æš‚å­˜æ—¥å¿—çš„ç›®å½•
#                 "path": "/temp/logs",
#                 "absolute_path": False,  # æ˜¯å¦æ˜¯ç»å¯¹è·¯å¾„
#             }
#         }
#
#         # åˆ›å»ºæ‰§è¡Œå™¨
#         from EventMainActuator import Event, get_actuator
#         _actuator_instance = get_actuator()  # ç¡®ä¿å®ä¾‹çš„è·å–
#
#         from LoggerInstructionLibrary import register_commands
#         register_commands()  # ç¡®ä¿æ³¨å†Œé¢å¤–å‘½ä»¤
#
#     # ================= æ ¸å¿ƒæ–¹æ³• =================
#
#     def verify_directory(self, level_name: str) -> bool:
#         """æ ¡éªŒæŒ‡å®šçº§åˆ«çš„ç›®å½•ç»“æ„"""
#         if level_name not in self.root_directory:
#             return False
#
#         config = self.root_directory[level_name]
#         return check_directory(
#             path=config["path"],
#             absolute_path=config["absolute_path"],
#             create_if_missing=True
#         )
#
#     def verify_all_directories(self) -> dict:
#         """æ ¡éªŒæ‰€æœ‰ç›®å½•ç»“æ„"""
#         results = {}
#         for level_name in self.root_directory:
#             results[level_name] = self.verify_directory(level_name)
#         return results
#
#     def _handle_directory_check(self):
#         """æ‰§è¡Œç›®å½•æ£€æŸ¥çš„æ ¸å¿ƒé€»è¾‘"""
#         check_results = self.verify_all_directories()
#
#         for level_name, success in check_results.items():
#             if not success:
#                 self._handle_directory_error(level_name)
#
#     def _handle_directory_error(self, level_name: str):
#         """å¤„ç†ç›®å½•æ ¡éªŒå¤±è´¥çš„æƒ…å†µ"""
#         print(f"Critical error: {level_name} directory validation failed")
#         # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é”™è¯¯å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚ï¼š
#         # 1. å°è¯•é‡æ–°åˆ›å»ºç›®å½•
#         # 2. åˆ‡æ¢åˆ°å¤‡ç”¨ç›®å½•
#         # 3. è§¦å‘è­¦æŠ¥é€šçŸ¥
#
#     # def found_file(self, path=""):
#     #     """
#     #     åŒ…è£…get_fileså‡½æ•°
#     #
#     #     :param path: é»˜è®¤æŒ‡å‘ self.root_directory["main"]["path"]
#     #     :return: è¿”å›çš„å†…å®¹å°†ä¼šæ˜¯ç”Ÿæˆå™¨
#     #     """
#     #     if not path:
#     #         path = self.root_directory["main"]["path"]
#     #     return get_files(path)
#
#     async def main_loop(self):
#         """ä¸»äº‹ä»¶å¤„ç†å™¨"""
#
#         # ä¸€. æ£€æŸ¥éœ€è¦ä½¿ç”¨çš„æ—¥å¿—æ–‡ä»¶å¤¹çš„æƒ…å†µ å¹¶è‡ªåŠ¨å¤„ç†
#
#         # 1. éœ€è¦é˜²æ­¢æ—¥å¿—æ–‡ä»¶å¤¹åœ¨ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­è¢«åˆ é™¤æˆ–æ˜¯è·¯å¾„ä¸å­˜åœ¨çš„é—®é¢˜
#         # ç›®å½•æ£€æŸ¥å¤„ç†
#         self._handle_directory_check()
#
#         # pending_files = self.found_file()
#         # for item in pending_files:
#         #     pass
#
#
# # ================= å…¨å±€é…ç½® =================
#
# _loggerOperator_instance = LoggerOperator()  # å…ˆåˆ›å»ºå®ä¾‹
#
#
# def get_actuator() -> LoggerOperator:
#     """è·å–å•ä¾‹å®ä¾‹çš„æ¨èæ–¹å¼"""
#     return _loggerOperator_instance